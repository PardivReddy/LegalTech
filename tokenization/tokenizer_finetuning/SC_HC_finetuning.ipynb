{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "59204f2e-0d48-44ad-8a72-198ac3bdc528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /home/infodna\n",
      "Dataset directory: /home/infodna/datasets/Predex_Dataset\n",
      "Tokenizer output directory: /home/infodna/new_legal_bert_tokenizer\n",
      "Final vocab file will be: /home/infodna/new_legal_bert_tokenizer/vocab.txt\n",
      "Available GPUs for training: [1, 2, 3, 4, 5, 7]\n",
      "Found existing vocab.txt at /home/infodna/new_legal_bert_tokenizer/vocab.txt, using it...\n",
      "‚úÖ Saved tokenizer configuration to /home/infodna/new_legal_bert_tokenizer/tokenizer.json\n",
      "‚úÖ Saved Hugging Face tokenizer configuration to /home/infodna/new_legal_bert_tokenizer\n",
      "‚úÖ Vocabulary saved to /home/infodna/new_legal_bert_tokenizer/vocab.txt\n",
      "Loading file: /home/infodna/datasets/Predex_Dataset/L-NLProc_PredEx_Instruction-Tuning_Prediction_test.csv\n",
      "Extracted 4868 text entries from L-NLProc_PredEx_Instruction-Tuning_Prediction_test.csv\n",
      "Loading file: /home/infodna/datasets/Predex_Dataset/L-NLProc_PredEx_Instruction_sets_train.csv\n",
      "Extracted 32 text entries from L-NLProc_PredEx_Instruction_sets_train.csv\n",
      "Loading file: /home/infodna/datasets/Predex_Dataset/L-NLProc_PredEx_Instruction-Tuning_Pred-Exp_train.csv\n",
      "Extracted 43844 text entries from L-NLProc_PredEx_Instruction-Tuning_Pred-Exp_train.csv\n",
      "Loading file: /home/infodna/datasets/Predex_Dataset/anuragiiser_PREDEX_train.csv\n",
      "Extracted 248 text entries from anuragiiser_PREDEX_train.csv\n",
      "Loading file: /home/infodna/datasets/Predex_Dataset/InLegal-AI-EXP_PredEx_Instruction-Tuning_Pred-Exp_train.csv\n",
      "Extracted 43844 text entries from InLegal-AI-EXP_PredEx_Instruction-Tuning_Pred-Exp_train.csv\n",
      "Loading file: /home/infodna/datasets/Predex_Dataset/L-NLProc_PredEx_test.csv\n",
      "Extracted 8879 text entries from L-NLProc_PredEx_test.csv\n",
      "Loading file: /home/infodna/datasets/Predex_Dataset/L-NLProc_PredEx_Instruction-Tuning_Prediction_train.csv\n",
      "Extracted 4868 text entries from L-NLProc_PredEx_Instruction-Tuning_Prediction_train.csv\n",
      "Loading file: /home/infodna/datasets/Predex_Dataset/InLegal-AI-EXP_PredEx_Instruction-Tuning_Prediction_train.csv\n",
      "Extracted 4868 text entries from InLegal-AI-EXP_PredEx_Instruction-Tuning_Prediction_train.csv\n",
      "Loading file: /home/infodna/datasets/Predex_Dataset/L-NLProc_PredEx_train.csv\n",
      "Extracted 80378 text entries from L-NLProc_PredEx_train.csv\n",
      "Loading file: /home/infodna/datasets/Predex_Dataset/InLegal-AI-EXP_PredEx_Instruction-Tuning_Prediction_test.csv\n",
      "Extracted 4868 text entries from InLegal-AI-EXP_PredEx_Instruction-Tuning_Prediction_test.csv\n",
      "Loading file: /home/infodna/datasets/Predex_Dataset/L-NLProc_PredEx_Instruction-Tuning_Pred-Exp_test.csv\n",
      "Extracted 4868 text entries from L-NLProc_PredEx_Instruction-Tuning_Pred-Exp_test.csv\n",
      "Loading file: /home/infodna/datasets/Predex_Dataset/InLegal-AI-EXP_PredEx_Instruction-Tuning_Pred-Exp_test.csv\n",
      "Extracted 4868 text entries from InLegal-AI-EXP_PredEx_Instruction-Tuning_Pred-Exp_test.csv\n",
      "‚úÖ Total text entries collected: 206433\n",
      "üîç Validating tokenizer...\n",
      "Sample text tokenized: 1 chunks\n",
      "Input IDs shape: torch.Size([1, 512])\n",
      "Attention mask shape: torch.Size([1, 512])\n",
      "üéâ Tokenizer saved and validated successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from tokenizers import BertWordPieceTokenizer, pre_tokenizers, processors\n",
    "\n",
    "# Directory setup\n",
    "base_dir = os.path.abspath(\"\")\n",
    "dataset_dir = os.path.join(base_dir, \"datasets\", \"Predex_Dataset\")\n",
    "output_dir = os.path.join(base_dir, \"new_legal_bert_tokenizer\")\n",
    "vocab_file = os.path.join(output_dir, \"vocab.txt\")\n",
    "\n",
    "print(f\"Current working directory: {base_dir}\")\n",
    "print(f\"Dataset directory: {dataset_dir}\")\n",
    "print(f\"Tokenizer output directory: {output_dir}\")\n",
    "print(f\"Final vocab file will be: {vocab_file}\")\n",
    "\n",
    "# Collect texts from PredEx dataset for validation\n",
    "def collect_texts(directory: str):\n",
    "    files = glob.glob(os.path.join(directory, \"*.csv\"))\n",
    "    texts = []\n",
    "\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No CSV files found in {directory}\")\n",
    "\n",
    "    for file in files:\n",
    "        print(f\"Loading file: {file}\")\n",
    "        df = pd.read_csv(file, low_memory=False)\n",
    "        string_data = df.select_dtypes(include=[\"object\"]).values.flatten().tolist()\n",
    "        string_data = [str(t).strip() for t in string_data if pd.notna(t) and str(t).strip()]\n",
    "        texts.extend(string_data)\n",
    "        print(f\"Extracted {len(string_data)} text entries from {os.path.basename(file)}\")\n",
    "\n",
    "    print(f\"‚úÖ Total text entries collected: {len(texts)}\")\n",
    "    return texts\n",
    "\n",
    "# Check available GPUs\n",
    "excluded_gpus = [0, 6]\n",
    "available_gpus = [i for i in range(torch.cuda.device_count()) if i not in excluded_gpus]\n",
    "print(f\"Available GPUs for training: {available_gpus}\")\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Check for existing vocab.txt\n",
    "if not os.path.exists(vocab_file):\n",
    "    print(f\"No vocab.txt found at {vocab_file}, loading Legal-BERT base vocab...\")\n",
    "    original_tokenizer = AutoTokenizer.from_pretrained(\"nlpaueb/legal-bert-base-uncased\")\n",
    "    vocab_files = original_tokenizer.save_vocabulary(output_dir)\n",
    "    print(f\"Saved original Legal-BERT vocab to: {vocab_files[0]}\")\n",
    "else:\n",
    "    print(f\"Found existing vocab.txt at {vocab_file}, using it...\")\n",
    "\n",
    "# Initialize BertWordPieceTokenizer with existing vocab\n",
    "tokenizer = BertWordPieceTokenizer(\n",
    "    vocab_file,\n",
    "    lowercase=True,\n",
    "    strip_accents=False,\n",
    "    clean_text=True\n",
    ")\n",
    "\n",
    "# Configure tokenizer\n",
    "special_tokens = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "tokenizer.post_processor = processors.TemplateProcessing(\n",
    "    single=\"[CLS] $A [SEP]\",\n",
    "    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
    "    special_tokens=[\n",
    "        (\"[CLS]\", tokenizer.token_to_id(\"[CLS]\")),\n",
    "        (\"[SEP]\", tokenizer.token_to_id(\"[SEP]\")),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Save complete tokenizer\n",
    "tokenizer.save(os.path.join(output_dir, \"tokenizer.json\"))\n",
    "print(f\"‚úÖ Saved tokenizer configuration to {os.path.join(output_dir, 'tokenizer.json')}\")\n",
    "\n",
    "# Save Hugging Face-compatible tokenizer\n",
    "hf_tokenizer = AutoTokenizer.from_pretrained(\"nlpaueb/legal-bert-base-uncased\")\n",
    "hf_tokenizer.vocab_files = {\"vocab_file\": vocab_file}\n",
    "hf_tokenizer.save_pretrained(output_dir)\n",
    "print(f\"‚úÖ Saved Hugging Face tokenizer configuration to {output_dir}\")\n",
    "print(f\"‚úÖ Vocabulary saved to {vocab_file}\")\n",
    "\n",
    "# Collect texts for validation\n",
    "texts = collect_texts(dataset_dir)\n",
    "\n",
    "# Validate tokenizer\n",
    "print(\"üîç Validating tokenizer...\")\n",
    "sample_text = texts[0] if texts else \"Sample Indian legal judgment text for testing.\"\n",
    "hf_tokenizer = AutoTokenizer.from_pretrained(output_dir)\n",
    "encoding = hf_tokenizer(\n",
    "    sample_text,\n",
    "    max_length=512,\n",
    "    stride=50,\n",
    "    truncation=True,\n",
    "    padding=\"max_length\",\n",
    "    return_tensors=\"pt\",\n",
    "    return_overflowing_tokens=True\n",
    ")\n",
    "device = \"cuda\" if torch.cuda.is_available() and available_gpus else \"cpu\"\n",
    "input_ids = encoding[\"input_ids\"].to(device)\n",
    "attention_mask = encoding[\"attention_mask\"].to(device)\n",
    "print(f\"Sample text tokenized: {len(encoding['input_ids'])} chunks\")\n",
    "print(f\"Input IDs shape: {input_ids.shape}\")\n",
    "print(f\"Attention mask shape: {attention_mask.shape}\")\n",
    "\n",
    "print(\"üéâ Tokenizer saved and validated successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1bdf6cbb-ead4-4822-aaa5-f8e790f7f516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /home/infodna\n",
      "Previous tokenizer directory: /home/infodna/new_legal_bert_tokenizer\n",
      "Dataset directory: /home/infodna/datasets/high_court_dataset/txt_new\n",
      "Tokenizer output directory: /home/infodna/high_court_legal_bert_tokenizer\n",
      "Loading previous vocab file: /home/infodna/new_legal_bert_tokenizer/vocab.txt\n",
      "Available GPUs: [1, 2, 3, 4, 5, 7]\n",
      "Using device for validation: cuda:1\n",
      "Current vocab size: 30522. Training to new size: 32000\n",
      "‚úÖ Found 48590 TXT files for training\n",
      "\n",
      "\n",
      "\n",
      "‚úÖ Tokenizer training completed\n",
      "‚úÖ Saved trained tokenizer model to: /home/infodna/high_court_legal_bert_tokenizer/vocab.txt\n",
      "‚úÖ Saved tokenizer configuration to /home/infodna/high_court_legal_bert_tokenizer/tokenizer.json\n",
      "‚úÖ Saved Hugging Face tokenizer to /home/infodna/high_court_legal_bert_tokenizer\n",
      "üîç Validating tokenizer...\n",
      "‚úÖ Collected 5 sample texts for validation\n",
      "Sample text tokenized into 1 chunks\n",
      "Input IDs shape: torch.Size([1, 512])\n",
      "Attention mask shape: torch.Size([1, 512])\n",
      "üéâ Tokenizer fine-tuned and validated successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "import torch\n",
    "from transformers import BertTokenizer, AutoTokenizer\n",
    "from tokenizers import BertWordPieceTokenizer, pre_tokenizers, processors\n",
    "\n",
    "# Directory setup\n",
    "base_dir = os.path.abspath(\"\")\n",
    "previous_output_dir = os.path.join(base_dir, \"new_legal_bert_tokenizer\")\n",
    "dataset_dir = os.path.join(base_dir, \"datasets\", \"high_court_dataset\", \"txt_new\")\n",
    "output_dir = os.path.join(base_dir, \"high_court_legal_bert_tokenizer\")\n",
    "previous_vocab_file = os.path.join(previous_output_dir, \"vocab.txt\")\n",
    "\n",
    "print(f\"Current working directory: {base_dir}\")\n",
    "print(f\"Previous tokenizer directory: {previous_output_dir}\")\n",
    "print(f\"Dataset directory: {dataset_dir}\")\n",
    "print(f\"Tokenizer output directory: {output_dir}\")\n",
    "print(f\"Loading previous vocab file: {previous_vocab_file}\")\n",
    "\n",
    "# Collect text files for training\n",
    "def collect_text_files(directory: str):\n",
    "    files = glob.glob(os.path.join(directory, \"*.txt\"))\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No TXT files found in {directory}\")\n",
    "    print(f\"‚úÖ Found {len(files)} TXT files for training\")\n",
    "    return files\n",
    "\n",
    "# Collect sample texts for validation (load a few to avoid memory issues)\n",
    "def collect_sample_texts(directory: str, num_samples: int = 5):\n",
    "    files = glob.glob(os.path.join(directory, \"*.txt\"))\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No TXT files found in {directory}\")\n",
    "    random.shuffle(files)\n",
    "    texts = []\n",
    "    for file in files[:num_samples]:\n",
    "        with open(file, 'r', encoding='utf-8') as f:\n",
    "            text = f.read().strip()\n",
    "            if text:\n",
    "                texts.append(text)\n",
    "    print(f\"‚úÖ Collected {len(texts)} sample texts for validation\")\n",
    "    return texts\n",
    "\n",
    "# Check available GPUs\n",
    "excluded_gpus = [0, 6]\n",
    "available_gpus = [i for i in range(torch.cuda.device_count()) if i not in excluded_gpus]\n",
    "print(f\"Available GPUs: {available_gpus}\")\n",
    "device = f\"cuda:{available_gpus[0]}\" if available_gpus and torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device for validation: {device}\")\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Load previous vocab\n",
    "if not os.path.exists(previous_vocab_file):\n",
    "    raise FileNotFoundError(f\"No vocab.txt found at {previous_vocab_file}\")\n",
    "\n",
    "# Initialize BertWordPieceTokenizer with previous vocab\n",
    "tokenizer = BertWordPieceTokenizer(\n",
    "    previous_vocab_file,\n",
    "    lowercase=True,\n",
    "    strip_accents=False,\n",
    "    clean_text=True\n",
    ")\n",
    "\n",
    "# Get current vocab size\n",
    "current_vocab_size = tokenizer.get_vocab_size()\n",
    "new_vocab_size = 32000  # Allow adding new tokens; adjust if needed\n",
    "print(f\"Current vocab size: {current_vocab_size}. Training to new size: {new_vocab_size}\")\n",
    "\n",
    "# Configure special tokens and processors\n",
    "special_tokens = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "tokenizer.post_processor = processors.TemplateProcessing(\n",
    "    single=\"[CLS] $A [SEP]\",\n",
    "    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
    "    special_tokens=[\n",
    "        (\"[CLS]\", tokenizer.token_to_id(\"[CLS]\")),\n",
    "        (\"[SEP]\", tokenizer.token_to_id(\"[SEP]\")),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Collect files and train tokenizer\n",
    "text_files = collect_text_files(dataset_dir)\n",
    "tokenizer.train(\n",
    "    files=text_files,\n",
    "    vocab_size=new_vocab_size,\n",
    "    special_tokens=special_tokens,\n",
    "    min_frequency=2,\n",
    "    show_progress=True,\n",
    "    wordpieces_prefix=\"##\",\n",
    "    limit_alphabet=1000\n",
    ")\n",
    "print(\"‚úÖ Tokenizer training completed\")\n",
    "\n",
    "# Save the trained tokenizer model (includes new vocab.txt)\n",
    "saved_files = tokenizer.save_model(output_dir)\n",
    "print(f\"‚úÖ Saved trained tokenizer model to: {saved_files[0]}\")  # vocab.txt\n",
    "\n",
    "# Save complete tokenizer configuration\n",
    "tokenizer.save(os.path.join(output_dir, \"tokenizer.json\"))\n",
    "print(f\"‚úÖ Saved tokenizer configuration to {os.path.join(output_dir, 'tokenizer.json')}\")\n",
    "\n",
    "# Load and save as Hugging Face BertTokenizer\n",
    "hf_tokenizer = BertTokenizer.from_pretrained(output_dir)\n",
    "hf_tokenizer.save_pretrained(output_dir)\n",
    "print(f\"‚úÖ Saved Hugging Face tokenizer to {output_dir}\")\n",
    "\n",
    "# Validate tokenizer with chunking\n",
    "print(\"üîç Validating tokenizer...\")\n",
    "samples = collect_sample_texts(dataset_dir)\n",
    "sample_text = samples[0] if samples else \"Sample High Court legal judgment text for testing.\"\n",
    "encoding = hf_tokenizer(\n",
    "    sample_text,\n",
    "    max_length=512,\n",
    "    stride=50,\n",
    "    truncation=True,\n",
    "    padding=\"max_length\",\n",
    "    return_tensors=\"pt\",\n",
    "    return_overflowing_tokens=True\n",
    ")\n",
    "input_ids = encoding[\"input_ids\"].to(device)\n",
    "attention_mask = encoding[\"attention_mask\"].to(device)\n",
    "print(f\"Sample text tokenized into {len(encoding['input_ids'])} chunks\")\n",
    "print(f\"Input IDs shape: {input_ids.shape}\")\n",
    "print(f\"Attention mask shape: {attention_mask.shape}\")\n",
    "\n",
    "print(\"üéâ Tokenizer fine-tuned and validated successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d7417d89-3270-469f-bb95-cf831323612e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /home/infodna\n",
      "Previous tokenizer directory: /home/infodna/new_legal_bert_tokenizer\n",
      "Dataset directory: /home/infodna/datasets/high_court_dataset/txt_new\n",
      "Tokenizer output directory: /home/infodna/high_court_legal_bert_tokenizer\n",
      "Loading previous vocab file: /home/infodna/new_legal_bert_tokenizer/vocab.txt\n",
      "Available GPUs: [1, 2, 3, 4, 5, 7]\n",
      "Using device for validation: cuda:1\n",
      "Current vocab size: 30522. Training to new size: 31022\n",
      "‚úÖ Found 48590 TXT files, filtered to 48590 English-only files\n",
      "\n",
      "\n",
      "\n",
      "‚úÖ Tokenizer training completed\n",
      "‚úÖ Cleaned up temporary files\n",
      "‚úÖ Saved trained tokenizer model to: /home/infodna/high_court_legal_bert_tokenizer/vocab.txt\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 133\u001b[0m\n\u001b[1;32m    131\u001b[0m vocab \u001b[38;5;241m=\u001b[39m hf_tokenizer\u001b[38;5;241m.\u001b[39mget_vocab()\n\u001b[1;32m    132\u001b[0m english_vocab \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m vocab\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m re\u001b[38;5;241m.\u001b[39mmatch(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m^[a-zA-Z0-9\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms.,!?]*$\u001b[39m\u001b[38;5;124m'\u001b[39m, k) \u001b[38;5;129;01mor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m special_tokens}\n\u001b[0;32m--> 133\u001b[0m \u001b[43mhf_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspecial_tokens\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Ensure special tokens are included\u001b[39;00m\n\u001b[1;32m    134\u001b[0m hf_tokenizer\u001b[38;5;241m.\u001b[39madd_tokens([k \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m english_vocab\u001b[38;5;241m.\u001b[39mkeys() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m special_tokens])  \u001b[38;5;66;03m# Add English tokens\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚úÖ Filtered vocab size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(hf_tokenizer\u001b[38;5;241m.\u001b[39mget_vocab())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m English-only tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/nyayaanumana/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:954\u001b[0m, in \u001b[0;36mSpecialTokensMixin.add_special_tokens\u001b[0;34m(self, special_tokens_dict, replace_additional_special_tokens)\u001b[0m\n\u001b[1;32m    951\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    953\u001b[0m added_tokens \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 954\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m \u001b[43mspecial_tokens_dict\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m():\n\u001b[1;32m    955\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mSPECIAL_TOKENS_ATTRIBUTES, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKey \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a special token\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    957\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'items'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "import re\n",
    "import torch\n",
    "from transformers import BertTokenizer, AutoTokenizer\n",
    "from tokenizers import BertWordPieceTokenizer, pre_tokenizers, processors\n",
    "\n",
    "# Directory setup\n",
    "base_dir = os.path.abspath(\"\")\n",
    "previous_output_dir = os.path.join(base_dir, \"new_legal_bert_tokenizer\")\n",
    "dataset_dir = os.path.join(base_dir, \"datasets\", \"high_court_dataset\", \"txt_new\")\n",
    "output_dir = os.path.join(base_dir, \"high_court_legal_bert_tokenizer\")\n",
    "previous_vocab_file = os.path.join(previous_output_dir, \"vocab.txt\")\n",
    "\n",
    "print(f\"Current working directory: {base_dir}\")\n",
    "print(f\"Previous tokenizer directory: {previous_output_dir}\")\n",
    "print(f\"Dataset directory: {dataset_dir}\")\n",
    "print(f\"Tokenizer output directory: {output_dir}\")\n",
    "print(f\"Loading previous vocab file: {previous_vocab_file}\")\n",
    "\n",
    "# Function to filter strictly English-only text\n",
    "def filter_english_text(text: str) -> str:\n",
    "    # Keep only ASCII letters, digits, and minimal punctuation\n",
    "    pattern = r'[^a-zA-Z0-9\\s.,!?]'  # Exclude all non-English characters\n",
    "    cleaned_text = re.sub(pattern, '', text)\n",
    "    cleaned_text = ' '.join(cleaned_text.split())\n",
    "    return cleaned_text if cleaned_text.strip() else None\n",
    "\n",
    "# Collect text files and filter for English-only content\n",
    "def collect_text_files(directory: str):\n",
    "    files = glob.glob(os.path.join(directory, \"*.txt\"))\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No TXT files found in {directory}\")\n",
    "    \n",
    "    english_texts = []\n",
    "    for file in files:\n",
    "        with open(file, 'r', encoding='utf-8') as f:\n",
    "            text = f.read().strip()\n",
    "            cleaned_text = filter_english_text(text)\n",
    "            if cleaned_text:\n",
    "                temp_file = os.path.join(output_dir, f\"temp_{os.path.basename(file)}\")\n",
    "                with open(temp_file, 'w', encoding='utf-8') as temp_f:\n",
    "                    temp_f.write(cleaned_text)\n",
    "                english_texts.append(temp_file)\n",
    "    \n",
    "    print(f\"‚úÖ Found {len(files)} TXT files, filtered to {len(english_texts)} English-only files\")\n",
    "    return english_texts\n",
    "\n",
    "# Collect sample texts for validation\n",
    "def collect_sample_texts(directory: str, num_samples: int = 5):\n",
    "    files = glob.glob(os.path.join(directory, \"*.txt\"))\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No TXT files found in {directory}\")\n",
    "    random.shuffle(files)\n",
    "    texts = []\n",
    "    for file in files[:num_samples]:\n",
    "        with open(file, 'r', encoding='utf-8') as f:\n",
    "            text = f.read().strip()\n",
    "            cleaned_text = filter_english_text(text)\n",
    "            if cleaned_text:\n",
    "                texts.append(cleaned_text)\n",
    "    print(f\"‚úÖ Collected {len(texts)} English-only sample texts for validation\")\n",
    "    return texts\n",
    "\n",
    "# Check available GPUs\n",
    "excluded_gpus = [0, 6]\n",
    "available_gpus = [i for i in range(torch.cuda.device_count()) if i not in excluded_gpus]\n",
    "print(f\"Available GPUs: {available_gpus}\")\n",
    "device = f\"cuda:{available_gpus[0]}\" if available_gpus and torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device for validation: {device}\")\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Load previous vocab\n",
    "if not os.path.exists(previous_vocab_file):\n",
    "    raise FileNotFoundError(f\"No vocab.txt found at {previous_vocab_file}\")\n",
    "\n",
    "# Initialize BertWordPieceTokenizer with previous vocab\n",
    "tokenizer = BertWordPieceTokenizer(\n",
    "    previous_vocab_file,\n",
    "    lowercase=True,\n",
    "    strip_accents=False,\n",
    "    clean_text=True\n",
    ")\n",
    "\n",
    "# Get current vocab size\n",
    "current_vocab_size = tokenizer.get_vocab_size()\n",
    "new_vocab_size = current_vocab_size + 500  # Allow limited new English tokens\n",
    "print(f\"Current vocab size: {current_vocab_size}. Training to new size: {new_vocab_size}\")\n",
    "\n",
    "# Configure special tokens and processors\n",
    "special_tokens = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "tokenizer.post_processor = processors.TemplateProcessing(\n",
    "    single=\"[CLS] $A [SEP]\",\n",
    "    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
    "    special_tokens=[\n",
    "        (\"[CLS]\", tokenizer.token_to_id(\"[CLS]\")),\n",
    "        (\"[SEP]\", tokenizer.token_to_id(\"[SEP]\")),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Collect and filter files, then train tokenizer\n",
    "text_files = collect_text_files(dataset_dir)\n",
    "if not text_files:\n",
    "    raise ValueError(\"No English-only text files available for training after filtering\")\n",
    "tokenizer.train(\n",
    "    files=text_files,\n",
    "    vocab_size=new_vocab_size,\n",
    "    special_tokens=special_tokens,\n",
    "    min_frequency=2,\n",
    "    show_progress=True,\n",
    "    wordpieces_prefix=\"##\",\n",
    "    limit_alphabet=100  # Restrictive to enforce English-only\n",
    ")\n",
    "print(\"‚úÖ Tokenizer training completed\")\n",
    "\n",
    "# Clean up temporary files\n",
    "for temp_file in text_files:\n",
    "    os.remove(temp_file)\n",
    "print(\"‚úÖ Cleaned up temporary files\")\n",
    "\n",
    "# Save the trained tokenizer model\n",
    "saved_files = tokenizer.save_model(output_dir)\n",
    "print(f\"‚úÖ Saved trained tokenizer model to: {saved_files[0]}\")  # vocab.txt\n",
    "\n",
    "# Load into Hugging Face BertTokenizer and filter vocabulary\n",
    "hf_tokenizer = BertTokenizer(vocab_file=os.path.join(output_dir, \"vocab.txt\"))\n",
    "vocab = hf_tokenizer.get_vocab()\n",
    "english_vocab = {k: v for k, v in vocab.items() if re.match(r'^[a-zA-Z0-9\\s.,!?]*$', k) or k in special_tokens}\n",
    "hf_tokenizer.add_special_tokens({\"additional_special_tokens\": special_tokens})  # Correctly pass as dictionary\n",
    "hf_tokenizer.add_tokens([k for k in english_vocab.keys() if k not in special_tokens])  # Add English tokens\n",
    "print(f\"‚úÖ Filtered vocab size: {len(hf_tokenizer.get_vocab())} English-only tokens\")\n",
    "\n",
    "# Save the filtered Hugging Face tokenizer\n",
    "hf_tokenizer.save_pretrained(output_dir)\n",
    "print(f\"‚úÖ Saved Hugging Face tokenizer with English-only vocab to {output_dir}\")\n",
    "\n",
    "# Validate tokenizer with chunking\n",
    "print(\"üîç Validating tokenizer...\")\n",
    "samples = collect_sample_texts(dataset_dir)\n",
    "sample_text = samples[0] if samples else \"Sample High Court legal judgment text for testing.\"\n",
    "encoding = hf_tokenizer(\n",
    "    sample_text,\n",
    "    max_length=512,\n",
    "    stride=50,\n",
    "    truncation=True,\n",
    "    padding=\"max_length\",\n",
    "    return_tensors=\"pt\",\n",
    "    return_overflowing_tokens=True\n",
    ")\n",
    "input_ids = encoding[\"input_ids\"].to(device)\n",
    "attention_mask = encoding[\"attention_mask\"].to(device)\n",
    "print(f\"Sample text tokenized into {len(encoding['input_ids'])} chunks\")\n",
    "print(f\"Input IDs shape: {input_ids.shape}\")\n",
    "print(f\"Attention mask shape: {attention_mask.shape}\")\n",
    "\n",
    "# Print decoded tokens to verify English-only\n",
    "decoded_tokens = hf_tokenizer.convert_ids_to_tokens(input_ids[0].tolist())\n",
    "print(f\"Sample decoded tokens: {decoded_tokens[:10]}\")  # Show first 10 tokens\n",
    "\n",
    "print(\"üéâ English-only tokenizer fine-tuned and validated successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958a7bca-3b1c-47df-adb0-a1a61be755a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nyayaanumana)",
   "language": "python",
   "name": "nyayaanumana"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
