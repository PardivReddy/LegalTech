{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5465245-018a-44de-a3f2-a0f11691e30c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÇ Collecting texts for DistrictCourt ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CSV -> district_only_clean1: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ DistrictCourt: 0 texts collected\n",
      "\n",
      "üìÇ Collecting texts for TribunalCourt ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CSV -> Tribunal_single_2020_2024: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ TribunalCourt: 0 texts collected\n",
      "\n",
      "üìÇ Collecting texts for DailyOrders ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CSV -> Dailyorder_single_2020_2024: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ DailyOrders: 0 texts collected\n",
      "\n",
      "üéØ Total new texts collected: 0\n",
      "üìÑ Training corpus saved at /home/infodna/custom_tokenizer_stage2/training_texts_stage2.txt\n",
      "\n",
      "üîë Loading base tokenizer...\n",
      "üöÄ Training tokenizer with vocab size = 48000\n",
      "\n",
      "\n",
      "\n",
      "‚úÖ Tokenizer saved at /home/infodna/custom_tokenizer_stage2\n",
      "\n",
      "üîç Validation sample:\n",
      "Input: This is a sample Indian legal judgment text for tokenizer validation.\n",
      "Tokens: ['[CLS]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[SEP]']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "# ----------------- Config -----------------\n",
    "base_dir = os.path.abspath(\"\")\n",
    "\n",
    "# Existing tokenizer (already fine-tuned on Predex + High Court)\n",
    "base_tokenizer_dir = os.path.join(base_dir, \"custom_tokenizer\")\n",
    "\n",
    "# Stage 2 tokenizer output\n",
    "output_dir = os.path.join(base_dir, \"custom_tokenizer_stage2\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# New datasets (CSV format)\n",
    "court_datasets = {\n",
    "    \"DistrictCourt\": os.path.join(base_dir, \"datasets\", \"district_only_clean1\"),\n",
    "    \"TribunalCourt\": os.path.join(base_dir, \"datasets\", \"Tribunal_single_2020_2024\"),\n",
    "    \"DailyOrders\": os.path.join(base_dir, \"datasets\", \"Dailyorder_single_2020_2024\"),\n",
    "}\n",
    "\n",
    "# ----------------- Function to collect texts -----------------\n",
    "def collect_texts(directory: str):\n",
    "    texts = []\n",
    "    csv_files = glob.glob(os.path.join(directory, \"*.csv\"))\n",
    "    for file in tqdm(csv_files, desc=f\"CSV -> {os.path.basename(directory)}\"):\n",
    "        try:\n",
    "            df = pd.read_csv(file, low_memory=False)\n",
    "            string_data = df.select_dtypes(include=[\"object\"]).values.flatten().tolist()\n",
    "            string_data = [str(t).strip() for t in string_data if pd.notna(t) and str(t).strip()]\n",
    "            texts.extend(string_data)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö† Error reading {file}: {e}\")\n",
    "    return texts\n",
    "\n",
    "# ----------------- Collect new texts -----------------\n",
    "all_texts = []\n",
    "for court_name, path in court_datasets.items():\n",
    "    print(f\"\\nüìÇ Collecting texts for {court_name} ...\")\n",
    "    court_texts = collect_texts(path)\n",
    "    print(f\"‚úÖ {court_name}: {len(court_texts)} texts collected\")\n",
    "    all_texts.extend(court_texts)\n",
    "\n",
    "print(f\"\\nüéØ Total new texts collected: {len(all_texts)}\")\n",
    "\n",
    "# Save to training corpus\n",
    "training_corpus_file = os.path.join(output_dir, \"training_texts_stage2.txt\")\n",
    "with open(training_corpus_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for line in all_texts:\n",
    "        f.write(line + \"\\n\")\n",
    "\n",
    "print(f\"üìÑ Training corpus saved at {training_corpus_file}\")\n",
    "\n",
    "# ----------------- Load existing tokenizer -----------------\n",
    "print(\"\\nüîë Loading base tokenizer...\")\n",
    "tokenizer = BertWordPieceTokenizer(\n",
    "    os.path.join(base_tokenizer_dir, \"vocab.txt\"),\n",
    "    lowercase=True,\n",
    ")\n",
    "\n",
    "# ----------------- Train tokenizer with new data -----------------\n",
    "VOCAB_SIZE = 48000   # between 45k‚Äì50k\n",
    "print(f\"üöÄ Training tokenizer with vocab size = {VOCAB_SIZE}\")\n",
    "\n",
    "tokenizer.train(\n",
    "    files=[training_corpus_file],\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    min_frequency=2,\n",
    "    special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    ")\n",
    "\n",
    "# ----------------- Save new tokenizer -----------------\n",
    "tokenizer.save_model(output_dir)\n",
    "print(f\"‚úÖ Tokenizer saved at {output_dir}\")\n",
    "\n",
    "# ----------------- Validation sample -----------------\n",
    "sample_text = \"This is a sample Indian legal judgment text for tokenizer validation.\"\n",
    "encoded = tokenizer.encode(sample_text)\n",
    "print(\"\\nüîç Validation sample:\")\n",
    "print(\"Input:\", sample_text)\n",
    "print(\"Tokens:\", encoded.tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46662d84-4a3a-4ca8-a300-6f9648c37e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîë Loading custom tokenizer...\n",
      "‚úÖ Loaded tokenizer\n",
      "üìä Dataset has 18208 rows. Columns: ['instruction', 'input', 'output', 'metadata']\n",
      "\n",
      "üöÄ Verifying tokenization on 3 samples...\n",
      "\n",
      "=== Sample 1 ===\n",
      "Original Text (first 120 chars): Judgment on 18th April, 2008.Bhaskar J.These two first appeals arise out of two cases under Section 18 of the Land Act, ...\n",
      "Token IDs (first 20): [2, 602, 121, 7421, 2087, 7, 2102, 9, 11112, 31, 9, 597, 805, 758, 908, 2554, 416, 88, 805, 965]\n",
      "Tokens (first 20): ['[CLS]', 'judgment', 'on', '18th', 'april', ',', '2008', '.', 'bhaskar', 'j', '.', 'these', 'two', 'first', 'appeals', 'arise', 'out', 'of', 'two', 'cases']\n",
      "Decoded back (first 120 chars): judgment on 18th april, 2008. bhaskar j. these two first appeals arise out of two cases under section 18 of the land act ...\n",
      "UNK tokens: 0\n",
      "\n",
      "=== Sample 2 ===\n",
      "Original Text (first 120 chars): Beaumont, C.J.This is an appeal from a decision of J. sitting in bankruptcy.The facts as found by the learned Judge, whi...\n",
      "Token IDs (first 20): [2, 114, 3742, 29147, 7, 24, 9, 31, 9, 208, 113, 148, 249, 205, 22, 504, 88, 31, 9, 7369]\n",
      "Tokens (first 20): ['[CLS]', 'be', '##au', '##mont', ',', 'c', '.', 'j', '.', 'this', 'is', 'an', 'appeal', 'from', 'a', 'decision', 'of', 'j', '.', 'sitting']\n",
      "Decoded back (first 120 chars): beaumont, c. j. this is an appeal from a decision of j. sitting in bankruptcy. the facts as found by the learned judge,  ...\n",
      "UNK tokens: 0\n",
      "\n",
      "=== Sample 3 ===\n",
      "Original Text (first 120 chars): The appellant has filed this appeal assailing the order of the learned Single Judge the probate civil petition filed und...\n",
      "Token IDs (first 20): [2, 85, 231, 149, 337, 208, 249, 7147, 85, 184, 88, 85, 357, 2320, 1239, 85, 9064, 1268, 281, 337]\n",
      "Tokens (first 20): ['[CLS]', 'the', 'appellant', 'has', 'filed', 'this', 'appeal', 'assailing', 'the', 'order', 'of', 'the', 'learned', 'single', 'judge', 'the', 'probate', 'civil', 'petition', 'filed']\n",
      "Decoded back (first 120 chars): the appellant has filed this appeal assailing the order of the learned single judge the probate civil petition filed und ...\n",
      "UNK tokens: 0\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import BertWordPieceTokenizer\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# ----------------- Config -----------------\n",
    "base_dir = os.path.abspath(\"\")\n",
    "tokenizer_dir = os.path.join(base_dir, \"final_tokenizer\")  # Folder containing vocab.txt, tokenizer.json, etc.\n",
    "lora_dataset_path = os.path.join(base_dir, \"datasets/dataset_lora1.jsonl\")  # JSONL dataset path\n",
    "\n",
    "max_length = 512\n",
    "stride = 128\n",
    "\n",
    "# Load custom tokenizer\n",
    "print(\"üîë Loading custom tokenizer...\")\n",
    "vocab_path = os.path.join(tokenizer_dir, \"vocab.txt\")\n",
    "tokenizer = BertWordPieceTokenizer(vocab_path, lowercase=True)\n",
    "print(\"‚úÖ Loaded tokenizer\")\n",
    "\n",
    "# Load a sample of LoRA dataset\n",
    "df = pd.read_json(lora_dataset_path, lines=True)\n",
    "print(f\"üìä Dataset has {len(df)} rows. Columns: {df.columns.tolist()}\")\n",
    "\n",
    "text_column = 'input'\n",
    "label_column = 'output'\n",
    "\n",
    "# Take first 3 samples\n",
    "samples = df[[text_column, label_column]].head(3).to_dict(orient='records')\n",
    "\n",
    "print(\"\\nüöÄ Verifying tokenization on 3 samples...\")\n",
    "for i, sample in enumerate(samples, 1):\n",
    "    text = sample[text_column]\n",
    "    label = sample[label_column]\n",
    "\n",
    "    print(f\"\\n=== Sample {i} ===\")\n",
    "    print(f\"Original Text (first 120 chars): {text[:120]}...\")\n",
    "    encoded = tokenizer.encode(text)\n",
    "\n",
    "    # IDs and tokens\n",
    "    ids = encoded.ids\n",
    "    tokens = encoded.tokens\n",
    "\n",
    "    print(\"Token IDs (first 20):\", ids[:20])\n",
    "    print(\"Tokens (first 20):\", tokens[:20])\n",
    "\n",
    "    # Decode back\n",
    "    decoded = tokenizer.decode(ids)\n",
    "    print(\"Decoded back (first 120 chars):\", decoded[:120], \"...\")\n",
    "\n",
    "    # UNK check\n",
    "    unk_id = tokenizer.token_to_id(\"[UNK]\")\n",
    "    unk_count = sum(1 for _id in ids if _id == unk_id)\n",
    "    print(f\"UNK tokens: {unk_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3de683-c885-4ebf-a8f3-d817d1c205e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nyayaanumana)",
   "language": "python",
   "name": "nyayaanumana"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
