{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "605fe662-fab9-480a-91db-4d3de0a10547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys in sample entry (line 1): ['orig_index', 'label', 'num_chunks', 'chunks']\n",
      "Sample entry: {'orig_index': 4390, 'label': 'Rejected', 'num_chunks': 2, 'chunks': [{'token_ids': [2, 35, 9, 31, 9, 514, 3583, 281, 149, 412, 11216, 93, 85, 184, 259, 769, 9, 19, 9, 3599, 479, 93, 766, 281, 110, 9, 18891, 88, 3599, 85, 1125, 52141, 15281, 88, 202, 149, 220, 668, 103, 114, 24044, 19997, 4460, 4323, 128, 88, 85, 3583, 281, 9, 85, 4482, 88, 85, 88, 85, 3583, 281, 113, 117, 124, 1905, 88, 85, 184, 259, 769, 9, 19, 9, 3599, 7, 85, 1690, 7, 999, 8492, 4149, 88, 7849, 3539, 7, 33057, 6034, 8823, 7, 147, 19171, 103, 341, 85, 88, 85, 435, 108, 398, 962, 672, 1280, 1595, 205, 85, 488, 88, 3223, 88, 22, 2784, 890, 88, 85, 184, 9, 129, 113, 1125, 117, 85, 1201, 88, 85, 1690, 12176, 93, 133, 5211, 3052, 85, 184, 93, 2250, 149, 5302, 2498, 332, 284, 8533, 7340, 22531, 6302, 108, 12555, 9000, 248, 653, 103, 85, 184, 88, 208, 197, 9, 93, 85, 11845, 88, 917, 88, 85, 3583, 281, 7, 2216, 343, 539, 108, 93, 1930, 103, 85, 2216, 7, 2751, 8, 1664, 1479, 103, 114, 337, 124, 85, 19495, 7, 1762, 33, 9, 35, 9, 5093, 48102, 93, 202, 129, 149, 220, 3240, 3709, 117, 1188, 12, 962, 306, 1060, 220, 479, 121, 142, 9, 14, 9, 3593, 121, 85, 88, 85, 435, 259, 2199, 3520, 85, 390, 88, 85, 435, 9, 890, 88, 85, 184, 149, 278, 220, 4092, 103, 85, 2751, 8, 1664, 126, 1609, 439, 8, 13, 103, 9498, 10027, 655, 85, 470, 129, 149, 278, 220, 3709, 93, 85, 2751, 8, 1664, 117, 749, 85, 184, 306, 1060, 220, 943, 93, 2250, 7, 1104, 88, 1062, 147, 991, 1769, 165, 85, 1864, 88, 85, 184, 259, 205, 85, 1317, 88, 85, 372, 7, 129, 113, 2071, 117, 85, 3583, 281, 147, 121, 971, 9, 14, 9, 3593, 108, 444, 85, 584, 113, 6554, 205, 85, 8185, 88, 85, 3348, 308, 93, 85, 2751, 8, 1664, 7, 129, 433, 897, 117, 85, 3583, 281, 1479, 103, 114, 337, 451, 85, 184, 88, 85, 197, 306, 1060, 220, 943, 93, 2250, 108, 124, 208, 14810, 7, 222, 85, 515, 88, 88, 85, 3583, 281, 7, 129, 433, 897, 117, 286, 147, 110, 4323, 2892, 4321, 128, 5268, 88, 85, 3583, 281, 9, 3039, 1270, 1898, 3570, 2112, 40, 1379, 205, 22, 17234, 677, 85, 372, 108, 129, 113, 117, 85, 281, 147, 677, 2643, 20619, 3201, 199, 6568, 7, 2401, 121, 14, 9, 15, 9, 3593, 578, 3649, 103, 85, 435, 3778, 85, 1685, 88, 917, 88, 85, 2751, 8, 1664, 124, 85, 3997, 430, 30, 9, 26, 9, 121, 129, 433, 897, 205, 85, 1317, 88, 85, 2751, 1664, 117, 129, 147, 1973, 6937, 117, 85, 184, 259, 769, 9, 19, 9, 3599, 306, 1060, 220, 11205, 93, 2250, 4154, 126, 88, 85, 435, 147, 108, 148, 5576, 88, 349, 9, 466, 10, 8, 147, 4201, 165, 1108, 30, 9, 26, 9, 4199, 165, 294, 3487, 2497, 108, 103, 6907, 208, 7814, 7, 890, 88, 85, 184, 259, 142, 9, 14, 9, 3593, 479, 93, 3], 'start_idx': 0, 'end_idx': 512}, {'token_ids': [205, 22, 17234, 677, 85, 372, 108, 129, 113, 117, 85, 281, 147, 677, 2643, 20619, 3201, 199, 6568, 7, 2401, 121, 14, 9, 15, 9, 3593, 578, 3649, 103, 85, 435, 3778, 85, 1685, 88, 917, 88, 85, 2751, 8, 1664, 124, 85, 3997, 430, 30, 9, 26, 9, 121, 129, 433, 897, 205, 85, 1317, 88, 85, 2751, 1664, 117, 129, 147, 1973, 6937, 117, 85, 184, 259, 769, 9, 19, 9, 3599, 306, 1060, 220, 11205, 93, 2250, 4154, 126, 88, 85, 435, 147, 108, 148, 5576, 88, 349, 9, 466, 10, 8, 147, 4201, 165, 1108, 30, 9, 26, 9, 4199, 165, 294, 3487, 2497, 108, 103, 6907, 208, 7814, 7, 890, 88, 85, 184, 259, 142, 9, 14, 9, 3593, 479, 93, 3], 'start_idx': 384, 'end_idx': 512}]}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Path to your tokenized file\n",
    "tokenized_data_path = \"/home/infodna/Court-MOE/tokenization/Tokenized_output/tokenized_multi_fixed.jsonl\"\n",
    "\n",
    "# Read the first non-empty line\n",
    "with open(tokenized_data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            print(f\"Line {i+1} is empty, skipping...\")\n",
    "            continue\n",
    "        try:\n",
    "            sample = json.loads(line)\n",
    "            print(f\"Keys in sample entry (line {i+1}): {list(sample.keys())}\")\n",
    "            print(f\"Sample entry: {sample}\")\n",
    "            break  # Stop after the first valid entry\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Line {i+1} has malformed JSON, skipping...\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc318187-4f31-45fc-9ac6-89746528bed5",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Court-MOE/encoding/metadata_augmented_district_daily.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCourt-MOE/encoding/metadata_augmented_district_daily.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(data))\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(data[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mkeys())\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/serialization.py:1484\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1481\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m   1482\u001b[0m     pickle_load_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1484\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m   1485\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m   1486\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m   1487\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m   1488\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m   1489\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/serialization.py:759\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    757\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer: FileLike, mode: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _opener[IO[\u001b[38;5;28mbytes\u001b[39m]]:\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 759\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    760\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    761\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/serialization.py:740\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    739\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: Union[\u001b[38;5;28mstr\u001b[39m, os\u001b[38;5;241m.\u001b[39mPathLike[\u001b[38;5;28mstr\u001b[39m]], mode: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 740\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Court-MOE/encoding/metadata_augmented_district_daily.pth'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "data = torch.load(\"/home/infodna/Court-MOE/encoding/metadata_augmented_district_daily.pth\", map_location='cpu')\n",
    "print(len(data))\n",
    "print(data[0].keys())\n",
    "print(data[0]['embeddings'].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2dff9cc7-258d-466f-9221-94da08d733e1",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/infodna/Court-MOE/tokenization/Tokenized_output/tokenized_multi_fixed.jsonl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m max_token_id, max_segment_id, total_lines \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      5\u001b[0m bad_lines \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, line \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(f):\n\u001b[1;32m      9\u001b[0m         obj \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(line)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/infodna/Court-MOE/tokenization/Tokenized_output/tokenized_multi_fixed.jsonl'"
     ]
    }
   ],
   "source": [
    "\n",
    "import json\n",
    "\n",
    "path = \"Court-MOE/tokenization/Tokenized_output/tokenized_multi_fixed.jsonl\"\n",
    "max_token_id, max_segment_id, total_lines = -1, -1, 0\n",
    "bad_lines = []\n",
    "\n",
    "with open(path) as f:\n",
    "    for i, line in enumerate(f):\n",
    "        obj = json.loads(line)\n",
    "        ids = obj.get(\"input_ids\", [])\n",
    "        types = obj.get(\"token_type_ids\", [])\n",
    "        if ids:\n",
    "            max_token_id = max(max_token_id, max(ids))\n",
    "        if types:\n",
    "            max_segment_id = max(max_segment_id, max(types))\n",
    "        if any(t > 1 for t in types):\n",
    "            bad_lines.append(i)\n",
    "        total_lines += 1\n",
    "\n",
    "print(f\"Total lines: {total_lines}\")\n",
    "print(f\"Max token ID: {max_token_id}\")\n",
    "print(f\"Max segment ID: {max_segment_id}\")\n",
    "print(f\"Lines with segment_id > 1: {len(bad_lines)}\")\n",
    "if bad_lines:\n",
    "    print(f\"Example bad lines: {bad_lines[:10]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59dd768f-3130-4cc8-bc78-32b19b7f7fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/infodna/.local/lib/python3.10/site-packages/torch/_subclasses/functional_tensor.py:279: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:81.)\n",
      "  cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing /home/infodna/Court-MOE/encoded_output/checkpoint_10000.pth\n",
      "âœ… Loaded 80000 records\n",
      "Keys per record: ['embeddings', 'label', 'court_type_idx', 'case_id']\n",
      "Embedding length: 768\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch, glob, os\n",
    "for ckpt in sorted(glob.glob(\"/home/infodna/Court-MOE/encoded_output/checkpoint_*.pth\"))[:1]:\n",
    "    print(f\"Testing {ckpt}\")\n",
    "    data = torch.load(ckpt, map_location=\"cpu\")\n",
    "    print(f\"âœ… Loaded {len(data)} records\")\n",
    "    print(f\"Keys per record: {list(data[0].keys())}\")\n",
    "    print(f\"Embedding length: {len(data[0]['embeddings'])}\")\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3f5cf57-3518-4827-a35c-c9649ddd68b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting numpy\n",
      "  Using cached numpy-2.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "Using cached numpy-2.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n",
      "Installing collected packages: numpy\n",
      "\u001b[33m  WARNING: The scripts f2py and numpy-config are installed in '/home/infodna/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed numpy-2.2.6\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f8bedc-41de-4599-ba79-b85eefcb93e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "data = torch.load(\"/home/infodna/Court-MOE/encoded_output/encoded_chunks_all.pth\")\n",
    "for k, v in data.items():\n",
    "    if torch.is_tensor(v):\n",
    "        print(k, v.shape)\n",
    "    else:\n",
    "        print(k, len(v))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aed81fb5-f07a-41eb-821a-4a9155e60d63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Record 0 keys: ['orig_index', 'label', 'num_chunks', 'chunks']\n",
      "\n",
      "Record 1 keys: ['orig_index', 'label', 'num_chunks', 'chunks']\n",
      "\n",
      "Record 2 keys: ['orig_index', 'label', 'num_chunks', 'chunks']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "jsonl_path = \"/home/infodna/Court-MOE/tokenization/Tokenized_output/tokenized_multi_fixed.jsonl\"\n",
    "\n",
    "with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        sample = json.loads(line.strip())\n",
    "        if i < 3:  # show first 3 samples\n",
    "            print(f\"\\nRecord {i} keys:\", list(sample.keys()))\n",
    "            if \"court\" in sample: \n",
    "                print(\"  court:\", sample[\"court\"])\n",
    "            elif \"court_type\" in sample:\n",
    "                print(\"  court_type:\", sample[\"court_type\"])\n",
    "        else:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4063ecb5-a7af-41e9-a195-6b5fe786baa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ Loading dataset from: /home/infodna/Court-MOE/Datasets/dataset_multi_lora.jsonl\n",
      "âš ï¸ Detected JSONL format. Reading first few lines...\n",
      "âœ… Loaded 5 sample record(s).\n",
      "\n",
      "ğŸ§© Keys in first record: ['instruction', 'input', 'output', 'metadata']\n",
      "\n",
      "âš ï¸ No 'court' or 'court_type' fields found in any of the first few records.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# ğŸ§© CHECK IF LORA DATASET CONTAINS COURT INFORMATION\n",
    "# ============================================================\n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "# âš™ï¸ CONFIG: update path if needed\n",
    "lora_path = \"/home/infodna/Court-MOE/Datasets/dataset_multi_lora.jsonl\"  # or your actual LoRA dataset file\n",
    "\n",
    "# --- Safety check ---\n",
    "if not os.path.exists(lora_path):\n",
    "    raise FileNotFoundError(f\"âŒ File not found: {lora_path}\")\n",
    "\n",
    "print(f\"ğŸ“‚ Loading dataset from: {lora_path}\")\n",
    "\n",
    "# --- Load first few records ---\n",
    "records = []\n",
    "with open(lora_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    try:\n",
    "        data = json.load(f)  # single JSON file (list or dict)\n",
    "        if isinstance(data, list):\n",
    "            records = data[:5]\n",
    "        elif isinstance(data, dict):\n",
    "            records = [data]\n",
    "        else:\n",
    "            raise ValueError(\"Unknown JSON structure.\")\n",
    "    except json.JSONDecodeError:\n",
    "        # fallback for JSONL (line-by-line)\n",
    "        print(\"âš ï¸ Detected JSONL format. Reading first few lines...\")\n",
    "        with open(lora_path, \"r\", encoding=\"utf-8\") as f2:\n",
    "            for i, line in enumerate(f2):\n",
    "                if i >= 5:\n",
    "                    break\n",
    "                records.append(json.loads(line.strip()))\n",
    "\n",
    "# --- Print structure ---\n",
    "if not records:\n",
    "    print(\"âŒ No records found.\")\n",
    "else:\n",
    "    print(f\"âœ… Loaded {len(records)} sample record(s).\")\n",
    "    print(\"\\nğŸ§© Keys in first record:\", list(records[0].keys()))\n",
    "\n",
    "    # --- Check for court field ---\n",
    "    found = False\n",
    "    for i, rec in enumerate(records):\n",
    "        for key in rec.keys():\n",
    "            if \"court\" in key.lower():\n",
    "                print(f\"\\nRecord {i} has court field â†’ {key}: {rec[key]}\")\n",
    "                found = True\n",
    "    if not found:\n",
    "        print(\"\\nâš ï¸ No 'court' or 'court_type' fields found in any of the first few records.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98561055-99f1-422c-8d13-78d1a828b339",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "# Load encoded file\n",
    "encoded_file = \"/home/infodna/Court-MOE/encoded_output/encoded_chunks_all.pth\"\n",
    "data = torch.load(encoded_file)\n",
    "\n",
    "# Assuming each record has a 'court_type' field\n",
    "courts = [record['court_type'] for record in data]\n",
    "court_counts = pd.Series(courts).value_counts()\n",
    "print(court_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd7e4ad0-92bf-4953-b952-cf9f98266d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ Checking file: /home/infodna/Court-MOE/tokenization/Tokenized_output/tokenized_multi_fixed_reclassified.jsonl\n",
      "\n",
      "ğŸ§© Sample top-level keys: ['orig_index', 'label', 'num_chunks', 'chunks', 'court_type']\n",
      "\n",
      "âœ… Detected: This is a **TOKENIZED JSONL** file (contains token_ids inside chunks).\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# ğŸ§  DETECT FILE TYPE â€” TOKENIZED OR TEXT (LoRA)\n",
    "# ============================================================\n",
    "\n",
    "import json\n",
    "\n",
    "# âš™ï¸ Path to your JSONL file (update this as needed)\n",
    "jsonl_path = \"/home/infodna/Court-MOE/tokenization/Tokenized_output/tokenized_multi_fixed_reclassified.jsonl\"\n",
    "# Example alternative for tokenized file:\n",
    "# jsonl_path = \"/home/infodna/Court-MOE/tokenization/Tokenized_output/tokenized_multi_fixed_with_courts.jsonl\"\n",
    "\n",
    "print(f\"ğŸ“‚ Checking file: {jsonl_path}\\n\")\n",
    "\n",
    "token_ids_present = False\n",
    "text_fields_present = False\n",
    "first_keys = None\n",
    "\n",
    "with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i >= 5:  # read first few lines only\n",
    "            break\n",
    "        try:\n",
    "            obj = json.loads(line.strip())\n",
    "            if first_keys is None:\n",
    "                first_keys = list(obj.keys())\n",
    "\n",
    "            # Case 1: Tokenized structure\n",
    "            if \"chunks\" in obj:\n",
    "                for c in obj.get(\"chunks\", []):\n",
    "                    if isinstance(c, dict) and \"token_ids\" in c:\n",
    "                        token_ids_present = True\n",
    "\n",
    "            # Case 2: Text structure (LoRA style)\n",
    "            if any(k in obj for k in [\"instruction\", \"input\", \"output\"]):\n",
    "                text_fields_present = True\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Error reading line {i+1}: {e}\")\n",
    "            continue\n",
    "\n",
    "# --- Print summary ---\n",
    "print(f\"ğŸ§© Sample top-level keys: {first_keys}\\n\")\n",
    "\n",
    "if token_ids_present:\n",
    "    print(\"âœ… Detected: This is a **TOKENIZED JSONL** file (contains token_ids inside chunks).\")\n",
    "elif text_fields_present:\n",
    "    print(\"âœ… Detected: This is a **TEXT (LoRA) JSONL** file (contains instruction/input/output fields).\")\n",
    "else:\n",
    "    print(\"âš ï¸ Could not clearly detect â€” may be empty or corrupted. Check manually.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce318044-46a9-43c2-9f9b-e16582919f94",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'orjson'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# ============================================================\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# ğŸ§  MERGE TOKENIZED FILE WITH RECLASSIFIED LORA DATA\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# ============================================================\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\u001b[38;5;241m,\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01morjson\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcollections\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Counter\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# âš™ï¸ Paths\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'orjson'"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# ğŸ§  MERGE TOKENIZED FILE WITH RECLASSIFIED LORA DATA\n",
    "# ============================================================\n",
    "\n",
    "import os, orjson\n",
    "from collections import Counter\n",
    "\n",
    "# âš™ï¸ Paths\n",
    "base_dir = \"/home/infodna/Court-MOE/tokenization/Tokenized_output\"\n",
    "tokenized_path = os.path.join(base_dir, \"tokenized_multi_fixed.jsonl\")                # token_ids file\n",
    "reclassified_path = os.path.join(base_dir, \"tokenized_multi_fixed_reclassified.jsonl\")  # lora-labeled file\n",
    "output_path = os.path.join(base_dir, \"tokenized_multi_fixed_final.jsonl\")\n",
    "\n",
    "print(f\"ğŸ“‚ Loading files:\\n  Tokenized â†’ {tokenized_path}\\n  Reclassified â†’ {reclassified_path}\")\n",
    "\n",
    "# ============================================================\n",
    "# STEP 1 â€” Load all court_type info from reclassified file\n",
    "# ============================================================\n",
    "court_map = {}\n",
    "counts = Counter()\n",
    "total = 0\n",
    "\n",
    "with open(reclassified_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        if not line.strip():\n",
    "            continue\n",
    "        total += 1\n",
    "        obj = orjson.loads(line)\n",
    "        idx = str(obj.get(\"orig_index\"))\n",
    "        court = obj.get(\"court_type\", \"Unknown\")\n",
    "        if court:\n",
    "            court_map[idx] = court\n",
    "            counts[court] += 1\n",
    "        if total % 100000 == 0:\n",
    "            print(f\"ğŸ”„ Indexed {total:,} reclassified records...\")\n",
    "\n",
    "print(f\"\\nâœ… Indexed {len(court_map):,} records from reclassified file.\")\n",
    "for k, v in counts.items():\n",
    "    print(f\"  ğŸ› {k:<15} â†’ {v:,}\")\n",
    "\n",
    "# ============================================================\n",
    "# STEP 2 â€” Patch tokenized file using court_map\n",
    "# ============================================================\n",
    "patched, total_lines = 0, 0\n",
    "final_counts = Counter()\n",
    "\n",
    "with open(tokenized_path, \"r\", encoding=\"utf-8\") as fin, open(output_path, \"wb\") as fout:\n",
    "    for line in fin:\n",
    "        if not line.strip():\n",
    "            continue\n",
    "        total_lines += 1\n",
    "        obj = orjson.loads(line)\n",
    "        idx = str(obj.get(\"orig_index\"))\n",
    "        if idx in court_map:\n",
    "            obj[\"court_type\"] = court_map[idx]\n",
    "            patched += 1\n",
    "        else:\n",
    "            obj[\"court_type\"] = \"Unknown\"\n",
    "\n",
    "        final_counts[obj[\"court_type\"]] += 1\n",
    "        fout.write(orjson.dumps(obj) + b\"\\n\")\n",
    "\n",
    "        if total_lines % 100000 == 0:\n",
    "            print(f\"ğŸ”§ Processed {total_lines:,} â€” Patched: {patched:,}\")\n",
    "\n",
    "print(f\"\\nâœ… Merge complete! Saved â†’ {output_path}\")\n",
    "print(f\"ğŸ› Patched {patched:,} / {total_lines:,} records ({(patched/total_lines)*100:.2f}%)\")\n",
    "\n",
    "print(\"\\nğŸ“Š Final Court Distribution:\")\n",
    "for k, v in final_counts.items():\n",
    "    print(f\"  {k:<15} â†’ {v:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af7b9b94-6ce3-420b-98c5-d97754e57528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Checked 200 records.\n",
      "\n",
      "ğŸ§© Metadata keys found:\n",
      "  - filename (200 occurrences)\n",
      "  - court_type (200 occurrences)\n",
      "\n",
      "ğŸ›ï¸ Example 'court_type' values (sample):\n",
      "  â€¢ SupremeCourt\n",
      "  â€¢ HighCourt\n",
      "\n",
      "ğŸ“‚ Example 'filename' values (sample):\n",
      "  â€¢ SupremeCourt_1960_50\n",
      "  â€¢ SupremeCourt_2002_796\n",
      "  â€¢ Bombay_HC_1995_121\n",
      "  â€¢ Delhi_HC_2005_771\n",
      "  â€¢ Jharkhand_HC_2003_1105\n",
      "  â€¢ Himachal_HC_2018_1093\n",
      "  â€¢ Punjab_Harayana_HC_2007_886\n",
      "  â€¢ Delhi_District_Court_2007_2020_2019_1854\n",
      "  â€¢ Consumer_Disputes_2016_1607\n",
      "  â€¢ karnataka_HC_2014_4393\n",
      "\n",
      "ğŸ“„ Example metadata structure (first record):\n",
      "\n",
      "{\n",
      "  \"filename\": \"SupremeCourt_1960_50\",\n",
      "  \"court_type\": \"SupremeCourt\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# ğŸ§  INSPECT METADATA STRUCTURE (FOR TOKENIZER USAGE)\n",
    "# ============================================================\n",
    "\n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "# âš™ï¸ Path to your dataset\n",
    "file_path = \"/home/infodna/Court-MOE/Datasets/dataset_multi_lora_reclassified_final.jsonl\"\n",
    "\n",
    "# --- Initialize ---\n",
    "metadata_keys = Counter()\n",
    "sample_records = []\n",
    "court_examples = []\n",
    "filename_examples = []\n",
    "total = 0\n",
    "\n",
    "# --- Read first 100 lines safely ---\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        if not line.strip():\n",
    "            continue\n",
    "        total += 1\n",
    "        try:\n",
    "            obj = json.loads(line.strip())\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "        meta = obj.get(\"metadata\", {})\n",
    "        if isinstance(meta, dict):\n",
    "            metadata_keys.update(meta.keys())\n",
    "\n",
    "            if \"court_type\" in meta:\n",
    "                court_examples.append(meta[\"court_type\"])\n",
    "            if \"filename\" in meta:\n",
    "                filename_examples.append(meta[\"filename\"])\n",
    "\n",
    "        if len(sample_records) < 3:\n",
    "            sample_records.append(obj)\n",
    "\n",
    "        if total >= 200:  # just check a few hundred lines\n",
    "            break\n",
    "\n",
    "# --- Results ---\n",
    "print(f\"âœ… Checked {total:,} records.\\n\")\n",
    "\n",
    "print(\"ğŸ§© Metadata keys found:\")\n",
    "for k, v in metadata_keys.items():\n",
    "    print(f\"  - {k} ({v:,} occurrences)\")\n",
    "\n",
    "print(\"\\nğŸ›ï¸ Example 'court_type' values (sample):\")\n",
    "for c in set(court_examples[:10]):\n",
    "    print(f\"  â€¢ {c}\")\n",
    "\n",
    "print(\"\\nğŸ“‚ Example 'filename' values (sample):\")\n",
    "for f in filename_examples[:10]:\n",
    "    print(f\"  â€¢ {f}\")\n",
    "\n",
    "# --- Show one sample record ---\n",
    "print(\"\\nğŸ“„ Example metadata structure (first record):\\n\")\n",
    "if sample_records:\n",
    "    meta = sample_records[0].get(\"metadata\", {})\n",
    "    print(json.dumps(meta, indent=2))\n",
    "else:\n",
    "    print(\"âš ï¸ No records found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "671af869-e3fe-4710-a652-bdf1921b9670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ LORA Dataset : /home/infodna/Court-MOE/Datasets/dataset_multi_lora_reclassified_final.jsonl\n",
      "ğŸ“‚ Tokenized File: /home/infodna/Court-MOE/tokenization/Tokenized_output_pakka/tokenized_multi_fixed_1760646787.jsonl\n",
      "\n",
      "âœ… BASIC STATS\n",
      "-----------------\n",
      "LoRA records:      526,541\n",
      "Tokenized records: 526,541\n",
      "âš–ï¸  Match check: âœ… SAME\n",
      "\n",
      "ğŸ·ï¸  Label distribution (LoRA):\n",
      "  Rejected        â†’ 221,585\n",
      "  Accepted        â†’ 304,956\n",
      "\n",
      "ğŸ·ï¸  Label distribution (Tokenized):\n",
      "  Rejected        â†’ 221,585\n",
      "  Accepted        â†’ 304,956\n",
      "\n",
      "ğŸ›ï¸ Court distribution (Tokenized):\n",
      "  SupremeCourt    â†’ 228,600\n",
      "  HighCourt       â†’ 232,978\n",
      "  Unknown         â†’ 35,779\n",
      "  TribunalCourt   â†’ 25,246\n",
      "  DailyOrderCourt â†’ 641\n",
      "  DistrictCourt   â†’ 3,297\n",
      "\n",
      "ğŸ§© CONTENT CONSISTENCY CHECK (first few records)\n",
      "\n",
      "Record #0:\n",
      "ğŸ”¹ LoRA text (first 200 chars):\n",
      "Given the text of an Indian court judgment, predict the final decision as either Accepted or Rejected. CIVIL APPELLATE Civil Appeal No.408 of 1957.Appeal by Special Leave from the Judgment and Order d\n",
      "ğŸ”¹ Tokenized token_ids (first 20): [2, 606, 85, 4919, 88, 148, 1354, 197, 602, 7, 5361, 4100, 41, 85, 1188, 504, 126, 1487, 1323, 127]\n",
      "ğŸ”¹ Court: SupremeCourt, Label: Rejected\n",
      "\n",
      "Record #1:\n",
      "ğŸ”¹ LoRA text (first 200 chars):\n",
      "Given the text of an Indian court judgment, predict the final decision as either Accepted or Rejected. Civil Appeal arising out of SLP (C) No.19277/02 Impleadment allowed.Leave granted.This appeal by \n",
      "ğŸ”¹ Tokenized token_ids (first 20): [2, 606, 85, 4919, 88, 148, 1354, 197, 602, 7, 5361, 4100, 41, 85, 1188, 504, 126, 1487, 1323, 127]\n",
      "ğŸ”¹ Court: SupremeCourt, Label: Accepted\n",
      "\n",
      "Record #2:\n",
      "ğŸ”¹ LoRA text (first 200 chars):\n",
      "Given the text of an Indian court judgment, predict the final decision as either Accepted or Rejected. M. Lodha, J.By this petition, the petitioner is seeking to challenge the order passed by the Resi\n",
      "ğŸ”¹ Tokenized token_ids (first 20): [2, 606, 85, 4919, 88, 148, 1354, 197, 602, 7, 5361, 4100, 41, 85, 1188, 504, 126, 1487, 1323, 127]\n",
      "ğŸ”¹ Court: SupremeCourt, Label: Rejected\n",
      "\n",
      "ğŸ§¾ SUMMARY\n",
      "-----------------\n",
      "âœ… Record count match confirmed.\n",
      "âœ… Label sets match.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# ğŸ§  TOKENIZED FILE INTEGRITY + MATCH CHECK (LoRA vs Tokenized)\n",
    "# ============================================================\n",
    "\n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "# âš™ï¸ Paths\n",
    "base_dir = \"/home/infodna/Court-MOE\"\n",
    "lora_path = f\"{base_dir}/Datasets/dataset_multi_lora_reclassified_final.jsonl\"\n",
    "tokenized_path = f\"{base_dir}/tokenization/Tokenized_output_pakka/tokenized_multi_fixed_1760646787.jsonl\"\n",
    "\n",
    "print(f\"ğŸ“‚ LORA Dataset : {lora_path}\")\n",
    "print(f\"ğŸ“‚ Tokenized File: {tokenized_path}\\n\")\n",
    "\n",
    "# --- Basic counts ---\n",
    "lora_count, token_count = 0, 0\n",
    "lora_labels, token_labels = Counter(), Counter()\n",
    "court_counts = Counter()\n",
    "\n",
    "# --- Load small samples to verify ---\n",
    "lora_samples, token_samples = {}, {}\n",
    "\n",
    "# --- 1ï¸âƒ£ Read LoRA dataset ---\n",
    "with open(lora_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if not line.strip(): \n",
    "            continue\n",
    "        obj = json.loads(line)\n",
    "        lora_count += 1\n",
    "        if i < 5:\n",
    "            lora_samples[i] = obj\n",
    "        label = obj.get(\"output\", \"\").strip()\n",
    "        if label:\n",
    "            lora_labels[label] += 1\n",
    "\n",
    "# --- 2ï¸âƒ£ Read Tokenized dataset ---\n",
    "with open(tokenized_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if not line.strip(): \n",
    "            continue\n",
    "        obj = json.loads(line)\n",
    "        token_count += 1\n",
    "        if i < 5:\n",
    "            token_samples[i] = obj\n",
    "        label = obj.get(\"label\", \"\").strip()\n",
    "        if label:\n",
    "            token_labels[label] += 1\n",
    "        if \"court_type\" in obj:\n",
    "            court_counts[obj[\"court_type\"]] += 1\n",
    "\n",
    "# --- 3ï¸âƒ£ Summaries ---\n",
    "print(\"âœ… BASIC STATS\\n-----------------\")\n",
    "print(f\"LoRA records:      {lora_count:,}\")\n",
    "print(f\"Tokenized records: {token_count:,}\")\n",
    "print(f\"âš–ï¸  Match check: {'âœ… SAME' if lora_count == token_count else 'âš ï¸ MISMATCH'}\")\n",
    "\n",
    "print(\"\\nğŸ·ï¸  Label distribution (LoRA):\")\n",
    "for k, v in lora_labels.items():\n",
    "    print(f\"  {k:<15} â†’ {v:,}\")\n",
    "\n",
    "print(\"\\nğŸ·ï¸  Label distribution (Tokenized):\")\n",
    "for k, v in token_labels.items():\n",
    "    print(f\"  {k:<15} â†’ {v:,}\")\n",
    "\n",
    "print(\"\\nğŸ›ï¸ Court distribution (Tokenized):\")\n",
    "for k, v in court_counts.items():\n",
    "    print(f\"  {k:<15} â†’ {v:,}\")\n",
    "\n",
    "# --- 4ï¸âƒ£ Token ID / Text consistency sampling ---\n",
    "print(\"\\nğŸ§© CONTENT CONSISTENCY CHECK (first few records)\")\n",
    "for i in range(3):\n",
    "    print(f\"\\nRecord #{i}:\")\n",
    "    lora = lora_samples.get(i)\n",
    "    token = token_samples.get(i)\n",
    "\n",
    "    if not lora or not token:\n",
    "        print(\"âš ï¸ Missing sample data\")\n",
    "        continue\n",
    "\n",
    "    lora_text = \" \".join([lora.get(\"instruction\",\"\"), lora.get(\"input\",\"\"), lora.get(\"output\",\"\")]).strip()\n",
    "    token_chunks = token.get(\"chunks\", [])\n",
    "    first_chunk_ids = token_chunks[0][\"token_ids\"][:20] if token_chunks else []\n",
    "\n",
    "    print(f\"ğŸ”¹ LoRA text (first 200 chars):\\n{lora_text[:200]}\")\n",
    "    print(f\"ğŸ”¹ Tokenized token_ids (first 20): {first_chunk_ids}\")\n",
    "    print(f\"ğŸ”¹ Court: {token.get('court_type', 'N/A')}, Label: {token.get('label', 'N/A')}\")\n",
    "\n",
    "# --- 5ï¸âƒ£ Overall verdict ---\n",
    "print(\"\\nğŸ§¾ SUMMARY\\n-----------------\")\n",
    "if lora_count == token_count:\n",
    "    print(\"âœ… Record count match confirmed.\")\n",
    "else:\n",
    "    print(\"âš ï¸ Record count mismatch detected! Double-check tokenization completeness.\")\n",
    "\n",
    "label_diff = set(lora_labels.keys()) - set(token_labels.keys())\n",
    "if not label_diff:\n",
    "    print(\"âœ… Label sets match.\")\n",
    "else:\n",
    "    print(f\"âš ï¸ Missing labels in tokenized: {label_diff}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44716984-6770-4826-bb60-9ebcfb55c429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.57.1-py3-none-any.whl.metadata (43 kB)\n",
      "Collecting filelock (from transformers)\n",
      "  Using cached filelock-3.20.0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n",
      "  Using cached huggingface_hub-0.35.3-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/infodna/.local/lib/python3.10/site-packages (from transformers) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Using cached regex-2025.9.18-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.5)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
      "  Using cached tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Using cached safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.34.0->transformers)\n",
      "  Using cached fsspec-2025.9.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.14.1)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub<1.0,>=0.34.0->transformers)\n",
      "  Using cached hf_xet-1.1.10-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2025.8.3)\n",
      "Downloading transformers-4.57.1-py3-none-any.whl (12.0 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m91.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached huggingface_hub-0.35.3-py3-none-any.whl (564 kB)\n",
      "Using cached hf_xet-1.1.10-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "Using cached tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "Using cached fsspec-2025.9.0-py3-none-any.whl (199 kB)\n",
      "Using cached regex-2025.9.18-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (789 kB)\n",
      "Using cached safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached filelock-3.20.0-py3-none-any.whl (16 kB)\n",
      "Installing collected packages: tqdm, safetensors, regex, hf-xet, fsspec, filelock, huggingface-hub, tokenizers, transformers\n",
      "\u001b[?25l\u001b[33m  WARNING: The script tqdm is installed in '/home/infodna/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6/9\u001b[0m [huggingface-hub]\u001b[33m  WARNING: The scripts hf, huggingface-cli and tiny-agents are installed in '/home/infodna/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m\u001b[90mâ”â”â”â”\u001b[0m \u001b[32m8/9\u001b[0m [transformers]\u001b[33m  WARNING: The scripts transformers and transformers-cli are installed in '/home/infodna/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9/9\u001b[0m [transformers]\n",
      "\u001b[1A\u001b[2KSuccessfully installed filelock-3.20.0 fsspec-2025.9.0 hf-xet-1.1.10 huggingface-hub-0.35.3 regex-2025.9.18 safetensors-0.6.2 tokenizers-0.22.1 tqdm-4.67.1 transformers-4.57.1\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "606e4aeb-515b-4e24-a227-457f33d02b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded tokenizer: /home/infodna/Court-MOE/tokenization/final_tokenizer\n",
      "\n",
      "âœ… Loaded 5 tokenized records for inspection.\n",
      "\n",
      "ğŸ§© Keys present in tokenized file:\n",
      "dict_keys(['orig_index', 'label', 'court_type', 'num_chunks', 'chunks'])\n",
      "\n",
      "ğŸ“Š Important column values for first few records:\n",
      "\n",
      "Record 0:\n",
      "  orig_index  : 0\n",
      "  label       : Rejected\n",
      "  court_type  : SupremeCourt\n",
      "  num_chunks  : 2\n",
      "  chunks keys : ['token_ids', 'start_idx', 'end_idx']\n",
      "------------------------------------------------------------\n",
      "Record 1:\n",
      "  orig_index  : 1\n",
      "  label       : Accepted\n",
      "  court_type  : SupremeCourt\n",
      "  num_chunks  : 2\n",
      "  chunks keys : ['token_ids', 'start_idx', 'end_idx']\n",
      "------------------------------------------------------------\n",
      "Record 2:\n",
      "  orig_index  : 2\n",
      "  label       : Rejected\n",
      "  court_type  : SupremeCourt\n",
      "  num_chunks  : 2\n",
      "  chunks keys : ['token_ids', 'start_idx', 'end_idx']\n",
      "------------------------------------------------------------\n",
      "Record 3:\n",
      "  orig_index  : 3\n",
      "  label       : Accepted\n",
      "  court_type  : SupremeCourt\n",
      "  num_chunks  : 2\n",
      "  chunks keys : ['token_ids', 'start_idx', 'end_idx']\n",
      "------------------------------------------------------------\n",
      "Record 4:\n",
      "  orig_index  : 4\n",
      "  label       : Accepted\n",
      "  court_type  : HighCourt\n",
      "  num_chunks  : 2\n",
      "  chunks keys : ['token_ids', 'start_idx', 'end_idx']\n",
      "------------------------------------------------------------\n",
      "\n",
      "ğŸ§  TEXT VERIFICATION (LoRA vs decoded tokenized):\n",
      "\n",
      "ğŸ”¹ Record 0:\n",
      "ğŸ§¾ LoRA text (first 300 chars):\n",
      "Given the text of an Indian court judgment, predict the final decision as either Accepted or Rejected. CIVIL APPELLATE Civil Appeal No.408 of 1957.Appeal by Special Leave from the Judgment and Order dated the 28th 1955, of the former Bombay High Court in Income-tax No.5 of 1955.Sanat P. Mehta, S. N.\n",
      "ğŸ§¾ Decoded token text (first 300 chars):\n",
      "given the text of an indian court judgment, pred ic t the final decision as either accepted or rejected. civil appellate civil appeal no. 408 of 1957. appeal by special leave from the judgment and order dated the 28th 1955, of the former bombay high court in income - tax no. 5 of 1955. san at p. meh\n",
      "ğŸ› Court: SupremeCourt | Label: Rejected\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ğŸ”¹ Record 1:\n",
      "ğŸ§¾ LoRA text (first 300 chars):\n",
      "Given the text of an Indian court judgment, predict the final decision as either Accepted or Rejected. Civil Appeal arising out of SLP (C) No.19277/02 Impleadment allowed.Leave granted.This appeal by state of Tamil Nadu is directed against the impugned judgment dated 25th of 2002 of the High Court o\n",
      "ğŸ§¾ Decoded token text (first 300 chars):\n",
      "given the text of an indian court judgment, pred ic t the final decision as either accepted or rejected. civil appeal arising out of slp ( c ) no. 1927 7 / 02 impleadment allowed. leave granted. this appeal by state of tamil nadu is directed against the impugned judgment dated 25th of 2002 of the hi\n",
      "ğŸ› Court: SupremeCourt | Label: Accepted\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ğŸ”¹ Record 2:\n",
      "ğŸ§¾ LoRA text (first 300 chars):\n",
      "Given the text of an Indian court judgment, predict the final decision as either Accepted or Rejected. M. Lodha, J.By this petition, the petitioner is seeking to challenge the order passed by the Resident Deputy Collector, Chandrapur on confirming the order of the Rent Controller dated whereby permi\n",
      "ğŸ§¾ Decoded token text (first 300 chars):\n",
      "given the text of an indian court judgment, pred ic t the final decision as either accepted or rejected. m. lodha, j. by this petition, the petitioner is seeking to challenge the order passed by the resident deputy collector, chandrapur on confirming the order of the rent controller dated whereby pe\n",
      "ğŸ› Court: SupremeCourt | Label: Rejected\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# ğŸ” VERIFY TOKENIZED STRUCTURE + REVERSE CHECK DECODING\n",
    "# ============================================================\n",
    "\n",
    "import json\n",
    "from collections import Counter\n",
    "from transformers import BertTokenizerFast\n",
    "\n",
    "# âš™ï¸ Paths\n",
    "base_dir = \"/home/infodna/Court-MOE\"\n",
    "tokenizer_path = f\"{base_dir}/tokenization/final_tokenizer\"\n",
    "lora_path = f\"{base_dir}/Datasets/dataset_multi_lora_reclassified_final.jsonl\"\n",
    "tokenized_path = f\"{base_dir}/tokenization/Tokenized_output_pakka/tokenized_multi_fixed_1760646787.jsonl\"\n",
    "\n",
    "# âœ… Load tokenizer for reverse decoding\n",
    "tokenizer = BertTokenizerFast.from_pretrained(tokenizer_path, local_files_only=True)\n",
    "print(f\"âœ… Loaded tokenizer: {tokenizer_path}\\n\")\n",
    "\n",
    "# --- Read first few tokenized records ---\n",
    "tokenized_records = []\n",
    "with open(tokenized_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i >= 5:  # show first 5\n",
    "            break\n",
    "        tokenized_records.append(json.loads(line.strip()))\n",
    "\n",
    "print(f\"âœ… Loaded {len(tokenized_records)} tokenized records for inspection.\\n\")\n",
    "\n",
    "# --- Check structure (keys) ---\n",
    "print(\"ğŸ§© Keys present in tokenized file:\")\n",
    "print(tokenized_records[0].keys())\n",
    "\n",
    "# --- Show index / label / court fields ---\n",
    "print(\"\\nğŸ“Š Important column values for first few records:\\n\")\n",
    "for i, rec in enumerate(tokenized_records):\n",
    "    print(f\"Record {i}:\")\n",
    "    print(f\"  orig_index  : {rec.get('orig_index')}\")\n",
    "    print(f\"  label       : {rec.get('label')}\")\n",
    "    print(f\"  court_type  : {rec.get('court_type')}\")\n",
    "    print(f\"  num_chunks  : {rec.get('num_chunks')}\")\n",
    "    print(f\"  chunks keys : {list(rec.get('chunks', [{}])[0].keys()) if rec.get('chunks') else None}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "# --- Decode tokens back to text to check integrity ---\n",
    "print(\"\\nğŸ§  TEXT VERIFICATION (LoRA vs decoded tokenized):\")\n",
    "with open(lora_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    lora_samples = [json.loads(next(f).strip()) for _ in range(3)]\n",
    "\n",
    "for i, rec in enumerate(tokenized_records[:3]):\n",
    "    chunk = rec[\"chunks\"][0][\"token_ids\"]\n",
    "    decoded_text = tokenizer.decode(chunk, skip_special_tokens=True)\n",
    "    lora_text = \" \".join([\n",
    "        lora_samples[i].get(\"instruction\", \"\"),\n",
    "        lora_samples[i].get(\"input\", \"\"),\n",
    "        lora_samples[i].get(\"output\", \"\")\n",
    "    ])[:400]\n",
    "\n",
    "    print(f\"\\nğŸ”¹ Record {i}:\")\n",
    "    print(f\"ğŸ§¾ LoRA text (first 300 chars):\\n{lora_text[:300]}\")\n",
    "    print(f\"ğŸ§¾ Decoded token text (first 300 chars):\\n{decoded_text[:300]}\")\n",
    "    print(f\"ğŸ› Court: {rec.get('court_type')} | Label: {rec.get('label')}\")\n",
    "    print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb34c923-d273-4e53-a277-bc8d0214ef0f",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'encoding/encoded_output_final/aggregated_doc_embeddings_fixed.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding/encoded_output_final/aggregated_doc_embeddings_fixed.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# load the .pth file\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# check the structure\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mType of data:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mtype\u001b[39m(data))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/serialization.py:1484\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1481\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m   1482\u001b[0m     pickle_load_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1484\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m   1485\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m   1486\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m   1487\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m   1488\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m   1489\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/serialization.py:759\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    757\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer: FileLike, mode: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _opener[IO[\u001b[38;5;28mbytes\u001b[39m]]:\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 759\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    760\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    761\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/serialization.py:740\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    739\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: Union[\u001b[38;5;28mstr\u001b[39m, os\u001b[38;5;241m.\u001b[39mPathLike[\u001b[38;5;28mstr\u001b[39m]], mode: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 740\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'encoding/encoded_output_final/aggregated_doc_embeddings_fixed.pth'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# path to your file\n",
    "path = \"Court-MOE/encoding/encoded_output_final/aggregated_doc_embeddings_fixed.pth\"\n",
    "\n",
    "# load the .pth file\n",
    "data = torch.load(path, map_location='cpu')\n",
    "\n",
    "# check the structure\n",
    "print(\"Type of data:\", type(data))\n",
    "\n",
    "# if it's a list (most common case)\n",
    "if isinstance(data, list):\n",
    "    print(f\"Total records: {len(data)}\\n\")\n",
    "    print(\"---- First 5 Records ----\")\n",
    "    for i, record in enumerate(data[:5]):\n",
    "        print(f\"\\nRecord {i+1}:\")\n",
    "        if isinstance(record, dict):\n",
    "            for key, val in record.items():\n",
    "                if isinstance(val, torch.Tensor):\n",
    "                    print(f\"  {key}: Tensor(shape={tuple(val.shape)})\")\n",
    "                else:\n",
    "                    print(f\"  {key}: {val}\")\n",
    "        else:\n",
    "            print(record)\n",
    "\n",
    "# if it's a dict of lists or tensors\n",
    "elif isinstance(data, dict):\n",
    "    print(\"Keys found:\", list(data.keys()))\n",
    "    first_key = next(iter(data))\n",
    "    # print first few entries per key\n",
    "    for key, val in data.items():\n",
    "        print(f\"\\nKey: {key}\")\n",
    "        if isinstance(val, torch.Tensor):\n",
    "            print(f\"Tensor shape: {tuple(val.shape)}\")\n",
    "            print(\"First 5 entries:\\n\", val[:5])\n",
    "        elif isinstance(val, list):\n",
    "            print(\"List length:\", len(val))\n",
    "            print(\"First 5 entries:\\n\", val[:5])\n",
    "        else:\n",
    "            print(\"Type:\", type(val))\n",
    "else:\n",
    "    print(\"Unexpected data type:\", type(data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd39f7c6-5cb6-4409-96d5-92ac58b17213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting numpy\n",
      "  Using cached numpy-2.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "Using cached numpy-2.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n",
      "Installing collected packages: numpy\n",
      "\u001b[33m  WARNING: The scripts f2py and numpy-config are installed in '/home/infodna/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed numpy-2.2.6\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a261dd6a-a80b-4dc7-85b8-48c35496119c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/infodna/.local/lib/python3.10/site-packages/torch/_subclasses/functional_tensor.py:279: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:81.)\n",
      "  cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'encoding/encoded_output_final/by_court/SupremeCourt_embeddings.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding/encoded_output_final/by_court/SupremeCourt_embeddings.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mğŸ” Type:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mtype\u001b[39m(data))\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/serialization.py:1484\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1481\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m   1482\u001b[0m     pickle_load_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1484\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m   1485\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m   1486\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m   1487\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m   1488\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m   1489\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/serialization.py:759\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    757\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer: FileLike, mode: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _opener[IO[\u001b[38;5;28mbytes\u001b[39m]]:\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 759\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    760\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    761\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/serialization.py:740\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    739\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: Union[\u001b[38;5;28mstr\u001b[39m, os\u001b[38;5;241m.\u001b[39mPathLike[\u001b[38;5;28mstr\u001b[39m]], mode: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 740\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'encoding/encoded_output_final/by_court/SupremeCourt_embeddings.pth'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "data = torch.load(\"encoding/encoded_output_final/by_court/SupremeCourt_embeddings.pth\", map_location=\"cpu\")\n",
    "print(\"\\nğŸ” Type:\", type(data))\n",
    "if isinstance(data, dict):\n",
    "    print(\"Keys:\", list(data.keys()))\n",
    "else:\n",
    "    print(\"Sample content:\", data[:1] if isinstance(data, list) else str(data)[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e426046-78ed-4f71-9037-a569ad33c302",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c498227-fa89-42df-8fa1-92f9a4546d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/infodna/.local/lib/python3.10/site-packages/torch/_subclasses/functional_tensor.py:279: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:81.)\n",
      "  cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "path = \"/home/infodna/Court-MOE/routers/router_best/encoded_cases_final_balanced_minority.pth\"\n",
    "\n",
    "data = torch.load(path, map_location=\"cpu\")\n",
    "print(f\"Type: {type(data)} | Length: {len(data)}\")\n",
    "\n",
    "# Print first few elements to see structure\n",
    "for i, item in enumerate(data[:3]):   # print first 3\n",
    "    print(f\"\\nItem {i}: Type={type(item)}\")\n",
    "    if torch.is_tensor(item):\n",
    "        print(\"  Tensor shape:\", item.shape)\n",
    "    elif isinstance(item, (list, tuple)):\n",
    "        print(\"  Tuple length:\", len(item))\n",
    "        if len(item) >= 2:\n",
    "            print(\"    [0] type:\", type(item[0]), \" shape:\", getattr(item[0], 'shape', None))\n",
    "            print(\"    [1] type:\", type(item[1]), \" shape:\", getattr(item[1], 'shape', None))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
