{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b243cb6-f34f-45d0-a7ea-71ccb5b19787",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import os\n",
    "from transformers import BertTokenizer\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.multiprocessing import Pool, set_start_method\n",
    "import numpy as np\n",
    "\n",
    "print(\"Script started\")  # Debug: Confirm script starts\n",
    "\n",
    "# ----------------- Config -----------------\n",
    "base_dir = os.path.abspath(\"\")\n",
    "tokenizer_dir = os.path.join(base_dir, \"final_tokenizer\")  # Folder containing vocab.txt, tokenizer.json, etc.\n",
    "lora_dataset_path = os.path.join(base_dir, \"datasets/dataset_single_lora.jsonl\")  # Specific JSONL file\n",
    "output_dir = os.path.join(base_dir, \"tokenized_output\")  # Directory to save tokenized data\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Set visible GPUs (skip 0, 1, 5; use 2, 3, 4, 6, 7)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2,3,4,6,7\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"ðŸ”‘ Device in use: {device}\")  # Verification print statement\n",
    "num_gpus = torch.cuda.device_count()\n",
    "print(f\"ðŸ”‘ Using {num_gpus} GPUs: {torch.cuda.get_device_name(0)} et al.\")\n",
    "\n",
    "# Tokenizer parameters\n",
    "max_length = 512  # Maximum sequence length for BERT\n",
    "stride = 128      # Stride for overlapping chunks\n",
    "\n",
    "# Load the custom tokenizer (using transformers)\n",
    "print(\"ðŸ”‘ Loading custom tokenizer...\")\n",
    "try:\n",
    "    tokenizer = BertTokenizer.from_pretrained(tokenizer_dir)\n",
    "    print(\"âœ… Loaded tokenizer using transformers\")\n",
    "except Exception as e:\n",
    "    print(f\"âš  Error loading tokenizer: {e}\")\n",
    "    raise\n",
    "\n",
    "# Custom Dataset for GPU processing\n",
    "class LoRADataset(Dataset):\n",
    "    def __init__(self, samples):\n",
    "        self.samples = samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        return sample['input'], sample['output']\n",
    "\n",
    "# Tokenization function for GPU (defined at module level)\n",
    "def tokenize_chunk(chunk_samples):\n",
    "    chunk_data = []\n",
    "    for i, sample in enumerate(chunk_samples):  # Unpack sample dict directly\n",
    "        text = sample['input']\n",
    "        label = sample['output']\n",
    "        # Move tokenization to GPU\n",
    "        encoded = tokenizer.encode_plus(\n",
    "            text,\n",
    "            max_length=max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        ).to(device)\n",
    "        \n",
    "        # Chunking logic\n",
    "        chunks = []\n",
    "        input_ids = encoded['input_ids'][0]  # Shape: (max_length,)\n",
    "        for start in range(0, max_length, max_length - stride):\n",
    "            end = min(start + max_length, max_length)\n",
    "            chunk_ids = input_ids[start:end]\n",
    "            if torch.any(chunk_ids != 0):  # Skip padding\n",
    "                chunks.append({\n",
    "                    \"token_ids\": chunk_ids.cpu().numpy().tolist(),\n",
    "                    \"start_idx\": start,\n",
    "                    \"end_idx\": end\n",
    "                })\n",
    "        \n",
    "        chunk_data.append({\n",
    "            \"sample_index\": i + 1,\n",
    "            \"label\": label,\n",
    "            \"num_chunks\": len(chunks),\n",
    "            \"chunks\": chunks\n",
    "        })\n",
    "    return chunk_data\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    set_start_method('spawn', force=True)\n",
    "    print(\"Starting main process\")  # Debug: Confirm main process starts\n",
    "\n",
    "    try:\n",
    "        print(f\"Loading dataset from {lora_dataset_path}\")  # Debug: Confirm file access attempt\n",
    "        # Read JSONL file\n",
    "        valid_data = []\n",
    "        with open(lora_dataset_path, 'r', encoding='utf-8') as f:\n",
    "            for i, line in enumerate(f, 1):\n",
    "                try:\n",
    "                    obj = json.loads(line.strip())\n",
    "                    valid_data.append(obj)\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"âš  Line {i} is malformed: {e}. Skipping.\")\n",
    "                    continue\n",
    "        df = pd.DataFrame(valid_data)\n",
    "        total_samples = len(df)\n",
    "        print(f\"ðŸ“Š Loaded dataset with {total_samples} rows. Columns: {df.columns.tolist()}\")\n",
    "        if total_samples != 18208:\n",
    "            print(f\"âš  Expected 18,208 samples, but found {total_samples}. Proceeding anyway.\")\n",
    "        \n",
    "        # Validate columns\n",
    "        text_column = 'input'\n",
    "        label_column = 'output'\n",
    "        if text_column not in df.columns or label_column not in df.columns:\n",
    "            print(f\"âš  Columns {text_column} or {label_column} not found. Available columns: {df.columns.tolist()}\")\n",
    "        else:\n",
    "            samples = df[[text_column, label_column]].to_dict(orient='records')\n",
    "            \n",
    "            # Split samples into chunks for each GPU\n",
    "            def split_into_chunks(samples, num_chunks):\n",
    "                chunk_size = len(samples) // num_chunks\n",
    "                chunks = []\n",
    "                for i in range(num_chunks):\n",
    "                    start = i * chunk_size\n",
    "                    end = start + chunk_size if i < num_chunks - 1 else len(samples)\n",
    "                    chunks.append(samples[start:end])\n",
    "                return chunks\n",
    "            \n",
    "            # Parallel processing across GPUs\n",
    "            print(\"\\nðŸš€ Tokenizing entire dataset with chunking and saving to file...\")\n",
    "            sample_chunks = split_into_chunks(samples, num_gpus)\n",
    "            with Pool(processes=num_gpus) as pool:\n",
    "                results = pool.map(tokenize_chunk, sample_chunks)  # Pass only chunks\n",
    "            \n",
    "            # Combine results\n",
    "            all_tokenized_data = []\n",
    "            for chunk_results in results:\n",
    "                all_tokenized_data.extend(chunk_results)\n",
    "            \n",
    "            # Validate and save\n",
    "            if len(all_tokenized_data) == total_samples:\n",
    "                print(f\"âœ… Processed exactly {total_samples} samples.\")\n",
    "            else:\n",
    "                print(f\"âš  Processed {len(all_tokenized_data)} samples, expected {total_samples}.\")\n",
    "            \n",
    "            # Save to a JSON file\n",
    "            output_file = os.path.join(output_dir, \"tokenized_full_dataset.json\")\n",
    "            with open(output_file, 'w', encoding='utf-8') as f:\n",
    "                json.dump(all_tokenized_data, f, ensure_ascii=False, indent=2)\n",
    "            print(f\"ðŸ“„ Saved tokenized data for entire dataset to {output_file}\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"âš  File {lora_dataset_path} not found. Please check the path.\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš  Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884ab1bc-eb61-4228-bb98-496332c741bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nyayaanumana)",
   "language": "python",
   "name": "nyayaanumana"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
